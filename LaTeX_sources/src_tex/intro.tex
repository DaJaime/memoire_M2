\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
\markboth{Introduction}{Introduction}
\label{chap:introduction}
\vspace{5mm}
Nous sommes au quotidien en contact avec une multitude boites noires (c’est-à-dire un système cachant sa logique interne à l’utilisateur). L’utilisateur ne pouvant pas savoir ce qu’il se passe dans le système, de nombreuses questions se posent. Notamment sur la confiance à accorder en ces systèmes, mais aussi d’un point de vue éthique particulièrement sur la protection de la vie privée de l’utilisateur.\\

Avec l’avènement de l’intelligence artificielle dans l’aide à la prise de décision, la problématique de compréhension de ces systèmes est devenu un des enjeux majeurs de notre génération. En effet, comment peut-on être assuré que notre modèle est réellement fiable ? Notre modèle a-t-il héritée involontairement de biais présent dans ses jeux de données ? Quel degré de compréhension est nécessaire et pour quelle complexité ?\\
Ces questions sont très importantes dans le domaine médical par exemple où un faux négatif peut être très problématique, car à la différence des faux positifs il n'y a pas de vérification postérieure effectué par l'humain. Par ailleurs, nombreux sont les cas où une intelligence artificielle a hérité de biais involontaire présent dans ses jeux de données d'entraînement engendrant différents types de discriminations. Comme par exemple l'intelligence artificielle de recrutement de l'entreprise Amazon qui rejetait automatiquement les CVs contenant le mot "femme".\\

Depuis 2016, en Europe, le Règlement Général de l'Union européenne sur la Protection des Données (RGPD) impose un droit à l'explication des décisions algorithmique prise au sujet d'un utilisateur \cite{RGPDexplanRight}. Or les raisons des décisions de ces algorithmes étant très abstraites même pour la personne l'aillant créée, un problème se pose et les utilisateurs ne peuvent pas réellement utiliser ce droit à l'explication.\\

Autre les questions d'éthique et de protections des utilisateurs, expliquer ces boites noire serai d'une très grande aide pour les entreprises afin de créer des produits plus sûr, plus efficacement et les aider à mieux gérer les problèmes de responsabilité.\\
Pour toutes ces raisons, fournir une explication à un système de décision type boite noire est un sujet de parfaite actualité et suscitant de nombreuses attentes. Le "Gartner hype cycle for Emerging Technologies" de 2019 place ce sujet (Explainable AI) dans le pic des attentes et suppose une maturité dans les cinq à dix ans.\\ 

L’objectif de ce mémoire est donc de comprendre comment une intelligence artificielle et plus largement comment un système boite noire est amené à prendre une décision. Pour cela, nous allons analyser les différentes méthodes existantes dans la littérature et les appliquer à plusieurs cas précis de systèmes boites noires. Afin de les comparer, les agréger, les classifier et de mettre en place une méthodologie fournissant une prédiction interprétable et explicable.\\

Le reste de ce mémoire sera organisé en plusieurs parties. Tout d'abords, nous commencerons avec un état de l'art (Chapitre 1) divisé en deux parties : la première sera axée sur des définitions et présentations d'éléments conceptuel, tandis que la seconds traitera des différentes méthodes et outils existant afin d'expliquer différents types de boites noire. Ensuite, dans le Chapitre 2 nous commencerons par apporter une méthodologie permettant de trouver quelle(s) méthode(s) explicative utiliser en fonction d'un problème donnée (type de boite noire, accès au code source ?, type de données ...). La suite du chapitre 2 consistera donc à appliquer notre méthodologie afin de l'évaluer celons deux critères : sa capacité à rendre un système de décision compréhensible par l'homme ainsi que sa capacité a être utilisée dans une multitude de problématiques.

