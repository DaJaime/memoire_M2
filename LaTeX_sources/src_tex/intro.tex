\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
\markboth{Introduction}{Introduction}
\label{chap:introduction}
\vspace{5mm}
\paragraph{}Nous sommes au quotidien en contact avec une multitude de boites noires, c’est-à-dire un système cachant sa logique interne à l’utilisateur. L’utilisateur ne pouvant pas savoir ce qu’il se passe dans le système, de nombreuses questions se posent. Notamment sur la confiance à accorder en ces systèmes, mais aussi d’un point de vue éthique particulièrement sur les questions de discrimination et de protection de la vie privée de l’utilisateur.

\paragraph{}Avec l’avènement de l’intelligence artificielle dans l’aide à la prise de décision, la problématique de compréhension de ces systèmes est devenu un des enjeux majeurs de notre génération. En effet, comment peut-on être assuré que notre modèle est réellement fiable ? Notre modèle a-t-il involontairement héritée de biais présent dans ses jeux de données d'entraînement ? Quel degré de compréhension est-il nécessaire de fournir et pour quelle complexité ?\\
Ces questions sont très importantes dans le domaine médical par exemple où un faux négatif peut être très problématique, car à la différence des faux positifs il n'y a pas de vérification postérieure effectué par l'humain. Par ailleurs, nombreux sont les cas où une intelligence artificielle a hérité de biais involontaire présent dans ses jeux de données d'entraînement engendrant différents types de discriminations. Comme par exemple l'intelligence artificielle de recrutement de l'entreprise Amazon qui rejetait automatiquement les CVs contenant le mot "femme"\cite{amazonAi}.

\paragraph{}Depuis 2016, en Europe, le Règlement Général de l'Union européenne sur la Protection des Données (RGPD) impose un droit à l'explication des décisions algorithmiques prise au sujet d'un utilisateur \cite{RGPDexplanRight}. Or les raisons des décisions de ces algorithmes étant très abstraites même pour la personne l'aillant créée, un problème se pose et les utilisateurs ne peuvent pas réellement utiliser ce droit à l'explication.

\paragraph{}En plus des questions d'éthique et de protections des utilisateurs, expliquer ces boites noires serai d'une très grande aide pour les entreprises afin de créer des produits plus sûr, plus efficacement et les aider à mieux gérer les problèmes de responsabilité.\par
Pour toutes ces raisons, fournir une explication à un système de décision type boite noire est un sujet de parfaite actualité suscitant de nombreuses attentes et tables rondes. Le "Gartner hype cycle for Emerging Technologies" de 2019 place ce sujet (Explainable AI) dans le pic des attentes et suppose une maturité dans les cinq à dix ans.

\paragraph{}L’objectif de ce mémoire est donc de comprendre comment un système boite noire et plus spécifiquement comment une intelligence artificielle est amenée à prendre une décision. Pour cela, nous allons analyser les différentes méthodes existantes dans la littérature et les appliquer à plusieurs cas concrets. Afin de savoir si expliquer ces systèmes boite noire nous permettrait de résoudre les problèmes au-quels ils sont confrontés et pour lesquels ils sont tant critiqués. Les applications porteront donc sur les questions d'éthique, de fiabilité et de performance.\\

\paragraph{}Le reste de ce mémoire sera organisé en plusieurs parties. Tout d'abord, nous commencerons par contextualiser afin de bien comprendre les différentes problématiques et dérives existantes. Nous enchaînerons par la suite avec un état de l'art divisé en deux parties : la première sera axée sur des définitions et présentations d'éléments conceptuels portant sur l'interprétabilité et de l'explicabilité, tandis que la seconde traitera des différentes méthodes et outils existant afin d'expliquer différents types de boite noire. La suite consistera donc à appliquer ces méthodes afin de d'évaluer leurs capacités à rendre un système de décision compréhensible par l'homme et de voir si ces explications permettent de palier aux problématiques décrites précédemment.

