\chapter{État de l’art}
\section{Interprétabilité et explicabilité}

Dans le domaine du machine learning, les mots interprétabilité et explicabilité sont très souvent utilisés conjointement et il peut être difficile au premier abords d'en saisir la différence. De plus, il existe de nombreuses définitions différentes pour ces termes et tous les experts ne s'accordent pas forcement à ce sujet. Après l'analyse de différentes propositions, nous nous accorderons, dans ce mémoire, à donner les définitions suivantes :\par
L'interprétabilité est le fait de pouvoir observer l'effet d'une cause dans un système c'est-à-dire de pouvoir prédire les changements dans la sortie lorsque l'on change une variable d'entrée.\par
L'explicabilité, quant à elle, est le fait d'expliquer dans des termes compréhensibles par l'homme la mécanique interne de notre système.\par
Afin de bien saisir la différence, le site \textit{KDnuggets} dans un article\cite{kdInterpre} nous invite à voir les choses comme si nous faisons une expérience scientifique à l'école : L'expérience peut être interprétable dans la mesure où vous pouvez voir ce que vous faites, mais elle n'est vraiment explicable qu'une fois que vous avez creusé la chimie derrière ce que vous pouvez voir se produire.\par
Créer un modèle interprétable implique de prendre en compte plusieurs facteurs :\par
\begin{description}
\item[Interprétabilité globale et local] L'interprétabilité d'un modèle est dite \textit{globale}, lorsque l'on comprend sa logique dans la totalité, nous sommes donc en mesure d'en expliquer toutes les solutions. À contrario, elle sera dite \textit{local}, lorsqu'il est possible d'expliquer seulement une ou plusieurs solutions spécifiques. Un problème complexe pourra ainsi être découpé en un sous problème plus simple afin de pouvoir expliquer une partie des solutions comme le montre la figure \ref{globalVSlocal} ci-dessous.
\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{src_img/globalVSlocal.png}
\caption{Un problème global complexe pouvant être expliqué à échelle local. \textit{Source : \cite{limePaper}}}
\label{globalVSlocal}
\end{figure}

\item[Limatation temporel] Le temps que l'on peut allouer pour fournir une explication à notre modèle est aussi à prendre en compte. En effet, fournir une explication peut prendre du temps et cette explication peut être longue à appréhender pour un être humain. Dans certains contextes où la prise de décision devra être effectuée rapidement, il sera préférable d'avoir une explication simple, compréhensible et fournis rapidement par la machine. Dans d'autres cas, nous pourrons prendre le temps d'aborder une explication plus complexe et détaillée.

\item[La cible de l'explication] La nature de l'expertise de l'utilisateur est aussi un facteur à prendre en compte dans le choix de l'explication que nous voulons lui apporter. En effets, un expert dans le domaine aura tendance à préférer une explication exhaustive et précises qu'une explication simple et opaque et inversement pour une personne moins à l'aise.
\end{description}

\subsection{Pré-requis d'un modèle interprétable}
En plus de prendre en compte les facteurs précédemment évoqués, un modèle interprétable doit être capable de satisfaire une liste de choses souhaitées. L'article "A survey of methods for explaining black box models"\cite{surveyExplaining} met en avant, après une analyse de différents états de l'art traitant de ce sujet, les desiderata d'un modèle interpretable :
\begin{description}
\item[Interpretabilité] Dans quelle mesure le modèle ou la prédiction sont compréhensifs par l'homme. Ce sujet est encore en débat afin de savoir comment mesurer cette interprétabilité.

\item[Précision] Dans quelle mesure le modèle interprétable prédit avec précision les différentes instances. La précision d'un modèle peut être faite avec le \textit{score de précision} (accuracy score), il s'agit simplement d'un rapport entre les observations correctement prédites et les observations totales. La précision est l'indicateur de base afin de calculer la précision d'un modèle, ce score fonctionne mieux si les faux positifs et les faux négatifs ont un coût similaire. Si le coût des faux positifs et des faux négatifs est très différent, il vaut mieux regarder le \textit{F1-score}. Le F1-score est la moyenne pondérée de la précision et du rappel.
\[
F1 Score = 2*\frac{Recall * Precision}{Recall + Precision}
\]
Où la \textit{précision} est le rapport des observations positives correctement prédites au total des observations positives prévues. Et le \textit{recall} (rappel) est le rapport des observations positives correctement prédites à toutes les observations dans la classe réelle.

\item[Éthique] Si notre modèle traite des données personnelles, il devra garantir en plus une protection contre toutes formes de discrimination ainsi qu'une protection de la vie privée des personnes concernées.\\
\end{description}

Ces différents aspects jouent un rôle important quant à la confiance qu'un utilisateur va apporter à notre modèle interprétable. De plus d'autres notions viennent s'ajouter, notamment pour les modèles d'exploration de données et d'apprentissage automatique. Il est important de respecter des critères tels que la \textit{robustesse}, la \textit{causalité}, l'\textit{évolutivité} et la \textit{généralité}. Cela signifie qu'un modèle doit garantir un certain niveau de performance indépendamment des données d'entrée (robustesse), et que les changements d'entrée dû à une perturbation affectent le comportement du modèle (causalité). Enfin, étant donné que nous pouvons utiliser le même modèle avec une multitude données d'entrée et dans différents cas d'applications, il est préférable d'avoir des modèles portables possédant un minimum de restriction (évolutivité, généralité).

\section{Ouvrir les boites noires}
L'article "A survey of methods for explaining black box models"\cite{surveyExplaining} passe en revue cinquante-quatre méthodes aillant pour but d'ouvrir différentes boites noires, en nous fournissant une classification exposée dans un tableau disponible en annexe 1 et 2. Nous aurons l'occasion de reparler de ce tableau plus tard. Cette revue permet de distinguer différents types de problèmes, d'explicateurs, de boites noires, de données d'entrée ainsi que différentes restrictions sur le modèle (accès au code, aux données...). Nous allons commencer par expliquer ces différentes caractéristiques.

\subsection{Types de problèmes}
\subsubsection{Explication du modèle}
Ce problème consiste à fournir un modèle interprétable et transparent capable d'imiter le comportement d'une boite noire et de nous fournir un prédicat compréhensible par l'homme.\\
Étant donnée un prédicateur de boite noire b et un ensemble de données D, le problème d'explication du modèle consiste à trouver une fonction f telle que f(b,D)=c où c est un prédicateur compréhensible capable d'imiter le comportement de b et dérivable afin d'obtenir une explication.

\subsubsection{Explication du résultats}
Ce problème consiste à fournir un résultat interprétable, c'est-à-dire que le modèle devra fournir le résultat avec une explication sur les raisons qui l'ont poussé à donner cette prédiction. Il n'est pas nécessaire d'expliquer la logique interne du système mais seulement le processus de décision pour une instance donnée (interprétation local).

\subsubsection{Inspection de la boîte noire}
Ce problème consiste à fournir une représentation visuelle ou textuelle afin de comprendre le fonctionnement interne de notre boite noire.\\
Étant donnée un prédicateur de boite noire b et un ensemble de données D, le problème d'explication du modèle consiste à trouver une fonction f telle que f(b,D)=V où V est une représentation du fonctionnement de la boite noire.

\subsubsection{Conception transparente}
Ce problème consiste à fournir un modèle transparent qui soit directement interprétable globalement ou localement.

\subsection{Types d'explicateurs}
\begin{itemize}
    \item \textbf{Arbre de décision (Decision Tree)} : L'arbre de décision exploite un arbre qui a pour noeuds des conditions, les arcs correspondent à la valeur d'une variable d'entrée et les feuilles correspondent aux différents labels possible. La figure \ref{decision_tree} montre un exemple trivial d'arbre de décision. Ainsi, la décision sera simplement expliquée en exprimant les chemins de l'arbre empruntés. 

    \begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{src_img/decision_tree.jpg}
    \caption{Exemple d'un arbre de décision}
    \label{decision_tree}
    \end{figure}
    
    \item \textbf{Règles de décision (Decion Rules)} : Utilisé pour expliquer le modèle, le résultat ainsi que pour la conception transparente. Il est aussi possible de transformer un arbre en un ensemble de règles. Les règles de décisions sont simplement des conditions IF-THEN : 
    
    if condition1, condition2, condition3 then outcome 
    
    \item \textbf{Importance des variables (Features Importance)} : Solution souvent utilisée, elle consiste à trouver les entrées de la boite noire pour lesquelles les poids sont les plus importants. Par exemple, pour une classification d'image, de trouver les pixels les plus importants dans l'entrée pour en arriver à une prédiction. Comme le montre la figure \ref{chickenPixel} tirée de l'article \textit{"Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models"} \cite{explainingIA}.\\
    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.35]{src_img/chickenPixel.png}
        \caption{Importance des fonctionnalités : explication de la prédiction "coq"}
        \label{chickenPixel}
    \end{figure}
    
    \item \textbf{Salient Mask} Généralement utilisé pour expliquer localement les réseaux de neurones profond (DNN), le salient mask permet de mettre en évidence visuellement les parties déterminantes de l'entrée analysée. L'article \textit{"Real Time Image Saliency for Black Box Classifiers"}\cite{silentMask} décrit son fonctionnement et la figure \ref{silentMaskExemple} montrant un exemple de salient mask est tirée de cet article.
    \begin{figure}[h]
        \centering
        \includegraphics[scale=0.85]{src_img/silentMaskExemple.PNG}
        \caption{Masque saillant : explication de la prédiction chien et chat}
        \label{silentMaskExemple}
    \end{figure}
    
    \item \textbf{Analyse de sensibilité (Sensitivity Analysis)} : Généralement utilisée pour l'inspection de boite noire. L'analyse de sensibilité consiste à évaluer l’incertitude statistique du résultat d’une boîte noire avec les différentes sources d’incertitude dans ses entrés. En d'autres termes, cela consiste à modifier des variables d'entrées afin de voir si cela a un impact sur le résultat en sorti et donc de savoir comment elles affectent notre prédiction.
    
    \item \textbf{Diagramme de dépendance partielle (Partial Dependence Plot)} : Ces graphiques permettent de comprendre l'effet d'une ou deux variables d'entrée sur la sortie du modèle. Le but étant de  montrer si la relation entre une caractéristique d'entrée et la sortie est linéaire, monotone ou plus complexe. Par exemple, appliqué à un modèle de régression linéaire, les tracés de dépendance partielle montrent toujours une relation linéaire. Nous somme limité par une ou deux variables à la fois, car une variable donne une représentation en 2 dimensions du problème et deux variables fournissent donc une représentation 3 en dimensions. Par exemple, la figure \ref{partialDependencePlot} tirée du livre \cite{molnar2019} montre trois diagrammes de dépendances différents sur trois valeurs d'entrées (la température, l'humidité et la vitesse du vent) et leurs impactes linéaire sur la prédiction du nombre de vélos.
    \begin{figure}[h]
        \includegraphics[scale=0.17]{src_img/partialDependencePlot.png}
        \caption{Exemple de diagramme de dépendance partielle.}
        \label{partialDependencePlot}
    \end{figure}
    
    \item \textbf{Sélection de prototype (Prototype Selection)} : Cet explicateur consiste à retourner, avec le résultat, un exemple très similaire à l'enregistrement classifié, afin de préciser avec quel critère la prédiction a été renvoyée.
    
    \item \textbf{Activation des neurones (Neurons Activation)} : L'analyse des réseaux de neurones permet aussi de comprendre son comportement. Cela consiste à analyser les neurones activés pour chaque entrés passées en argument à notre modèle.
\end{itemize}

L'explicateur varies en fonction de notre type de problème, de notre type de boite noire, de notre type de données d'entrée (images, texte...) et de nos différentes restrictions sur le modèle (accès au code, aux données...).

\section{Approches utilisées}
Face à ce besoin d'explicabilité grandissant, les entreprises peuvent préférer se tourner vers des modèles moins performant mais plus explicable afin de contourner la problématique de boite noire. Ainsi, nombreuse sont les entreprises qui se détournent de l'apprentissage profond au profil de l'apprentissage par arbre de décision ou les modèles linéaires par exemple qui fournissent un résultat plus compréhensibles et interprétable. Mais, la recherche dans ces domaines n'a pas évoluée depuis un certain temps et l'éventail des algorithmes disponibles est assez limité.

\subsubsection{Sur-couche explicative}
Le but étant d'essayer de fournir à l'utilisateur des éléments approximatifs permettant de comprendre son modèle boite noire, comme par exemple identifier les variables d'entrée les plus importantes dans la prise de décision du modèle. Comme le montre la figure \ref{explainCouche}, la sur-couche explicative vient se greffer après la prédiction du modèle.
\begin{figure}[h]
\centering
\includegraphics[scale=0.35]{src_img/explainCouche.png}
\caption{Position de la sur-couche explicative dans le processus de prédiction. \textit{Source : \cite{kdCouche}}}
\label{explainCouche}
\end{figure}
Différents outils proposent une approche permettant de livrer des éléments de compréhension approximatif pour un modèle boite noire simple. Nous allons présenter les deux outils les plus populaires, LIME et SHAP.
\subsection{LIME}
LIME signifie Local Interpretable Model-Agnostic Explanations (explications locales interprétables par modèle-agnostique). L'objectif est donc de fournir une interprétation local à un modèle de classification ou de régression (model agnostic) en se basant sur l’importance des variables comme vu précédemment. L'idée est de sonder la boite noire autant de fois que nécessaire en faisant varier les données d'entrées ce qui aura pour effet de produire une nouvelle sortie à chaque fois. Une fois toutes les perturbations sont effectuées il est alors possible de récupérer les coefficients qui composent la droite de la régression linéaire et ainsi de déduire l'importance des variables d'entrées. Par exemple dans la figure \ref{limeExemple} tirée de l'article original de la présentation de LIME\cite{limePaper} : L'image originelle (a) est donnée à notre boite noire qui prédit "Guitare électrique", "Guitare acoustique" et "Labrador". Nous constatons que le modèle s'est trompé sur la reconnaissance de la guitare électrique et nous utilisons LIME afin de comprendre ce qu'il s'est passé. LIME va prendre notre image (a) et la dériver de plusieurs façons en cachant certaines parties et envoyer ces nouvelles images à notre modèle. Le but étant de trouver les parties qui une fois cachées font que le modèle ne prédit plus la même chose. Puis une fois les trois classes trouvées, LIME nous envoie une explication où l'on voit les parties de l'image aillant joué un rôle dans la prédiction de chaque labels de notre modèle.
\begin{figure}[h]
\centering
\includegraphics[scale=0.35]{src_img/limeExemple.png}
\caption{Source : LIME Paper \cite{limePaper}}
\label{limeExemple}
\end{figure}

\subsection{SHAP}
Un an après la sortie de LIME, un nouvel outil fait son apparition afin de compenser ses quelques défauts, cet outil s'appelle SHAP. SHAP signifie SHapley Additive exPlanations, c'est une méthode permettant de fournir une interprétation globale ou local à notre modèle. Cette méthode est basée sur la valeur de Shapley issus de la théorie des jeux, il est donc nécessaire dans un premier temps d'expliquer brièvement cette valeur de Shapley.\par
La valeur de Shapley introduit par Shapley en 1953, permet de repartir les gains équitablement dans un jeu coopératif. Le but étant que tous les joueurs coopérant ensemble reçoivent un certain gain en fonction de leur contribution.\par
SHAP reprends cette idée afin d'expliquer toutes sortes de modèle de machine learning, le but étant d'associer une valeur égale à sa contribution dans la prédiction pour chaque caractéristique d'entrée. Prenons par exemple un modèle permettant de prédire le prix d'un logement. Une multitude de caractéristiques sont données en entrés à notre modèle afin d'estimer le prix du logement, le but de SHAP est donc de définir pour chacune de ces caractéristiques leur impact monétaire sur le prix final du logement. Il interrogera donc la boite noire avec une multitude de cas différents afin d'isoler le prix de chaque caractéristique. Prenons le cas de la figure \ref{shapleyExemple} où l'on veut expliquer la prévision de 310 000 euros pour un logement de 50m2, au premier étage, près d'un parc et où les animaux sont interdits. SHAP va donc recréer la même l'instance et la donner en entrée à la boite noire en autorisant les animaux afin d'en déduire le coût (dans notre exemple 10 000 euros). Cette opération sera effectuée sur toutes les caractéristiques du problème afin de trouver la contribution de chacune. Cela peut sembler trivial dans cet exemple mais il peut exister des dépendances entre les caractéristiques qui modifient leurs coût en fonction de la présence ou non d'une ou plusieurs autre(s) caractéristique(s).
\begin{figure}[h]
\centering
\includegraphics[scale=0.3]{src_img/shapleyExemple.png}
\caption{Exemple de l'explication du coût d'un logement avec SHAP. \textit{Source : \cite{molnar2019}}}
\label{shapleyExemple}
\end{figure}

\subsection{Limites de ces implémentations}
Aujourd'hui, de nombreuses implémentations de ces méthodes sont utilisées, mais elles essuient aussi quelques critiques et ne conviennent pas dans tous les cas de figures. Premièrement, ces algorithmes représentent un coût non négligeable. Ces méthodes basées sur des simulations et interrogeant notre modèle plusieurs fois pour une seule prédiction, peuvent poser des problèmes de performances lorsque l'on manipule un très grand nombre de données. Aussi la pertinence des explications fournies sont sujettes à débat, en effet ce sont des approximations et rien ne nous garantis que ces explications correspondent réellement au fonctionnement de notre modèle. Ces explications pourraient même donner l'effet inverse à celui escompté, en effet on peut arriver dans des cas où l'on fait confiance à notre modèle grâce à ces explications alors qu'elles ne sont pas fondées et ainsi prendre une décision critique appuyée sur une explication erroné.\par

Il est donc nécessaire d'utiliser ces algorithmes avec parcimonie et en aillant bien conscience qu'ils ne fournissent que des approximations de notre problème. Mais ce genre d'approches sont très bénéfiques pour la recherche, elles nous permettent d'apporter de nouvelles solutions ainsi que de mettre en lumière de nouvelles problématiques.
