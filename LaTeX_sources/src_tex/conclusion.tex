\chapter{Conclusion}

\paragraph{}Dans ce mémoire, nous sommes partis d'un constat général qui est que l'intelligence artificielle est souvent critiquée notamment pour des raisons d'éthique et de fiabilité. Nous avons émis l'hypothèse que ces problèmes venaient du fait que l'on ne comprend pas la logique interne de ces modèles et nous nous sommes demandé si comprendre un peu mieux ces boites noires nous permettrait de résoudre ces problématiques.

\paragraph{}Nous avons donc commencé par analyser les différentes méthodes existantes dans la littérature et définir les différents types d'explications et leurs pré requis. Ces recherches nous on permis de trouver deux outils fréquemment utilisés qui sont LIME et SHAP. Nous avons donc utilisé ces deux approches afin de voir si elles pouvaient nous permettre de détecter la présence de biais discriminatoire dans notre modèle et si elles pouvaient permettre d'en améliorer la fiabilité. Ces applications nous montrent le potentiel de l'explicabilité pour résoudre les problématiques liées à l'intelligence artificielle. Mais, comme nous le montre la section "Performance" du chapitre "Application" fournir une explication a un coût non négligeable qui mérite d'être pris en compte.

\paragraph{}Ce mémoire a donc pour objectif de promouvoir l'explication des modèles d'aide à la prise de décision. Accompagner chaque prédiction de son explication ou fournir une explication global à notre modèle aura pour effet de nous prémunir de toutes discriminations et d'améliorer la fiabilité de nos modèles. Aussi, cela permettrait d’augmenter la confiance de la population à l’égard de ces technologies et de contribuer à une évolution de la technologie plus saine.