# memoire_M2
## Abstract
Nous sommes au quotidien en contact avec une multitude boites noires (c’est-à-dire un système cachant sa logique interne à l’utilisateur). L’utilisateur ne pouvant pas savoir ce qu’il se passe dans le système, de nombreuses questions se posent. Notamment sur la confiance à accorder en ces systèmes, mais aussi d’un point de vue  ́ethique (particulièrement sur la protection de la vie privee de l’utilisateur).
Avec l’avènement de l’intelligence artificielle dans l’aide à la prise de décision, la problématique de compréhension de ces systèmes est devenu un des enjeux majeurs de notre génération. En effet, comment peut-on être assuré que notre modèle est réellement fiable ? Notre modèle a-t-il héritée involontairement de biais présent dans sa data-set ? Quel degré de compréhension est nécéssaire et pour quelle complexitée ?
L’objectif de ce mémoire est donc de comprendre comment une intelligence artificielle et plus largement comment un système de décision boite noire est amené à prendre une décision. Pour cela, nous allons analyser les différentes méthodes existantes dans la littérature et les appliquer à plusieurs cas précis de systèmes boites noires. Afin de les comparer, les agréger, les classifier et de mettre en place une méthodologie fournissant une prédiction interprétable et explicable.
